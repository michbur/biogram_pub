---
title: "biogram: N-Gram Analysis of Biological Sequences"
author: "Michal Burdukiewicz, Piotr Sobczyk"
date: "25.01.2015"
output:
  ioslides_presentation:
    widescreen: yes
  slidy_presentation: default
bibliography: biogram.bib
---

```{r echo=FALSE,message=FALSE}
library(ggplot2)
library(grid)
library(biogram)
library(knitr)
library(DT)
library(seqinr) 
library(dplyr)
#library(rbokeh)
library(reshape2)
library(DiagrammeR)
library(latex2exp)

size_mod <- -4
cool_theme <- theme(plot.background=element_rect(fill = "transparent",
                                                 colour = "transparent"),
                    panel.grid.major = element_line(colour="lightgrey", linetype = "dashed"),
                    panel.background = element_rect(fill = "white",colour = "black"),
                    legend.background = element_rect(fill="NA"),
                    legend.position = "right",
                    axis.text = element_text(size=12 + size_mod),
                    axis.title.x = element_text(size=16 + size_mod, vjust = -1), 
                    axis.title.y = element_text(size=16 + size_mod, vjust = 1),
                    strip.text = element_text(size=12 + size_mod, face = "bold"),
                    strip.background = element_rect(fill="grey", colour = "black"),
                    legend.text = element_text(size=13 + size_mod), 
                    legend.title = element_text(size=17 + size_mod),
                    plot.title = element_text(size=20 + size_mod))

options(DT.options = list(iDisplayLength = 6, searching = FALSE, bLengthChange = FALSE))
```

## Introduction

* curse of dimensionality,
* reduction of alphabets.

## n-grams

n-grams: continuous or discontinuous sub-sequences of nucleic or protein sequences.  

Applications:  
* determining the family of genomes to which a given genome belongs [@tomovic_n-gram-based_2006].  

## Reduction of alphabets

Features related to the 3d structure of proteins:  
* protein folding [@murphy_simplified_2000],  
* prediction of protein interactions [@launay_recognizing_2007].  

## Methods

* QuiPT (Fisher exact test, indefinitely faster, multiple criterions),
* normalized encoding distance (heuristic to find the best reduced alphabets).

## QuiPT

**Qui**ck **P**ermutation **T**est is a fast solution for filtering a large number of binary features provided 
that target vector is also binary. It is typical case for positioned n-gram data. 
Moreover, even considering unpositioned n-grams, if the $n$ is sufficiently high, 
the matrix of counts is almost sparse and little to no information is lost after 
data is artifically binarized.

## QuiPT

QuiPT uses criterions which measure how imbalanced is a contingency table.

In our application, analysis of n-grams, we are interested in Bernoulli r.v.
Let us consider the contingency table we get:

| target\\feature | 1 | 0 |
|:---:|:---:|:---:|
| 1 | $n_{1,1}$ | $n_{1,0}$ |
| 0 | $n_{0,1}$ | $n_{0,0}$ |


## QuiPT

If probability that target equals 1 is $p$ and probability that feature equals
1 is $q$ and feature and target are independant then each of them has the 
following probabilities 
$$P((Target, Feature) = (1,1)) = p \cdot q$$
$$P((Target, Feature) = (1,0)) = p \cdot (1-q)$$
$$P((Target, Feature) = (0,1)) = (1-p) \cdot q$$
$$P((Target, Feature) = (0,0)) = (1-p) \cdot (1-q)$$

This means that a target-feature can be described as multinomial distribution.

## QuiPT

Considering all above, we are able to compute exact probability of obtaining a specific confusion matrix depending on the $n_{1,1}$.

```{r, echo = FALSE, message = FALSE, results='asis'}
target_feature <- create_feature_target(10, 375, 15, 600) 
tmp_dist <- distr_crit(target = target_feature[,1], feature = target_feature[,2])
mtmp_dist <- melt(attr(tmp_dist, "plot_data"))
levels(mtmp_dist[["Var2"]]) <- c("Criterion", "Probability")

ggplot(mtmp_dist, aes(x = Var1, y = value, colour = Var2)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(TeX("$n_{1,1}$")) +
  scale_y_continuous("Value") +
  scale_color_discrete("") + 
  cool_theme
```

## Encoding distance

The encoding distance, as computed per the *calc_ed* function, between **a** and **b** is defined as the minimum number of amino acids that have to be moved between subgroups of encoding to make **a** identical to **b** (the order of subgroups in the encoding and amino acids in a group is unimportant).

## Encoding distance

The encoding distance may be further normalized by a factor reflecting how much moving amino acids between groups altered mean group properties. 

## Encoding A

```{r, echo = FALSE, message = FALSE, results='asis'}
group2df <- function(group_list, caption = NULL, label = NULL) {
  data.frame(ID = 1L:length(group_list), 
             Groups = sapply(group_list, function(i)
    paste0(toupper(sort(i)), collapse = ", ")))
}

a <- list(`1` = "p", 
          `2` = c("f", "i", "w", "y"), 
          `3` = c("a", "c", "d", "e", "g", "h", "k", "l", "m", "n", "q", "r", "s", "t", "v"))

kable(group2df(a), caption = "Encoding A")
```

## Encoding B

```{r, echo = FALSE, message = FALSE, results='asis'}
b <- list(`1` = c("f", "r", "w", "y"), 
          `2` = c("c", "i", "l", "t", "v"), 
          `3` = c("a", "d", "e", "g", "h", "k", "m", "n", "p", "q", "s"))

kable(group2df(b), caption = "Encoding B")
```


The encoding distance between both groups is `r calc_ed(a, b)`.  

## Normalized encoding distance 

```{r, echo = FALSE, message = FALSE, results='asis'}
data(aaprop)
a_prop <- aaprop[c(22, 211), ]

#b_prop <- aa_nprop[na.omit(traits_table[ao, ]), , drop = FALSE]

# must have unified lists of features

coords_a <- lapply(a, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))
coords_b <- lapply(b, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))

dat_a <- data.frame(enc = "a", do.call(rbind, coords_a), label = paste0("A", 1L:3))
dat_b <- data.frame(enc = "b", do.call(rbind, coords_b), label = paste0("B", 1L:3))

dat <- data.frame(do.call(rbind, lapply(1L:nrow(dat_a), function(id) 
  data.frame(id = id, rbind(do.call(rbind, lapply(1L:3, function(dummy) 
    dat_a[id, , drop = FALSE])),
    dat_b)))), pair = c(paste0("d", 1L:3), paste0("d", 1L:3)))

colnames(dat) <- c("id", "enc", "f1", "f2", "label", "pair")
dat[["id"]] <- paste0("Encoding a\nsubgroup ", dat[["id"]])


ggplot(dat, aes(x = f1, y = f2, colour = pair, label = label)) +
  geom_line() +
  geom_point(aes(x = f1, y = f2, colour = enc), size = 4) + 
  facet_wrap(~ id) + 
  geom_text(aes(x = f1, y = f2, colour = enc, label = label), vjust = 1.5, size = 4) + 
  scale_color_brewer(palette="Dark2", guide = "none") +
  cool_theme
```


## Normalized encoding distance 

The figure above represents the distances between groups of encoding **a** (green dots) and groups of encoding **b** (red dots). The position of the dot defined by mean values of specific properties of all amino acids belonging to the group.

## Normalized encoding distance 

```{r, echo = FALSE, message = FALSE, results='asis'}
data(aaprop)
a_prop <- aaprop[c(22, 211), ]

#b_prop <- aa_nprop[na.omit(traits_table[ao, ]), , drop = FALSE]

# must have unified lists of features

coords_a <- lapply(a, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))
coords_b <- lapply(b, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))

dat_a <- data.frame(enc = "a", do.call(rbind, coords_a), label = paste0("A", 1L:3))
dat_b <- data.frame(enc = "b", do.call(rbind, coords_b), label = paste0("B", 1L:3))

dat <- data.frame(do.call(rbind, lapply(1L:nrow(dat_a), function(id) 
  data.frame(id = id, rbind(do.call(rbind, lapply(1L:3, function(dummy) 
    dat_a[id, , drop = FALSE])),
    dat_b)))), pair = c(paste0("d", 1L:3), paste0("d", 1L:3)))

colnames(dat) <- c("id", "enc", "f1", "f2", "label", "pair")
dat[["id"]] <- paste0("Encoding a\nsubgroup ", dat[["id"]])


ggplot(dat, aes(x = f1, y = f2, colour = pair, label = label)) +
  geom_line() +
  geom_point(aes(x = f1, y = f2, colour = enc), size = 4) + 
  facet_wrap(~ id) + 
  geom_text(aes(x = f1, y = f2, colour = enc, label = label), vjust = 1.5, size = 4) + 
  scale_color_brewer(palette="Dark2", guide = "none") +
  cool_theme
```


## Normalized encoding distance 

```{r, echo = FALSE, message = FALSE, results='asis'}
tmp <- sapply(coords_a, function(single_coords_a) {
  distances <- sapply(coords_b, function(single_coords_b) 
    #vector of distances between groups
    sqrt(sum((single_coords_a - single_coords_b)^2))
  )
  #c(dist = min(distances), id = unname(which.min(distances)))
  distances
})

colnames(tmp) <- paste0("Enc a, group ", colnames(tmp))
rownames(tmp) <- paste0("Enc b, group ", rownames(tmp))

kable(tmp)
```


For each group in encoding **a**, we choose the minimum distance $d_i$ between the group and any group belonging to the encoding **b**. The normalization factor is equal to the sum of minimum distances for each group in **a**.

$$
n_f = \sum_{i \in 1, 2, 3} \min \left( d_i \right) 
$$

In the case depicted above, $n_f$ is equal to the sum of `r paste0(round(apply(tmp, 2, min), 4), collapse = ", ")`: `r round(sum(apply(tmp, 2, min)), 4)`.


## Results

Classification n-grams typical for various mitochondrial genomes.

Phy-Mer: a mitochondrial genome haplogroup-defining algorithm using a k-mer approach, useable also for NGS data [@navarro-gomez_phy-mer:_2015]

* long sequences of nucleotides of uneven length;  
* multiple classification problem.  

## Results

Recognizing features in epitopes [@koch_scrutinizing_2013]:  
* amino acid sequences,  
* equally long peptides (may use position information),  
* 8 residues,  
* only two categories.  

## References