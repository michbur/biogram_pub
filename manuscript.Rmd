---
title: "Quick Permutation Test for rapid filtering of n-gram data"
author: "Piotr Sobczyk, Paweł Mackiewicz and Michał Burdukiewicz"
date: "June 2016"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
csl: mee.csl
bibliography: biogram.bib
---

```{r echo=FALSE,message=FALSE}
library(ggplot2)
library(grid)
library(biogram)
library(knitr)
library(DT)
library(seqinr) 
library(dplyr)
library(reshape2)
library(latex2exp)

size_mod <- -4
cool_theme <- theme(plot.background=element_rect(fill = "transparent",
                                                 colour = "transparent"),
                    panel.grid.major = element_line(colour="lightgrey", linetype = "dashed"),
                    panel.background = element_rect(fill = "white",colour = "black"),
                    legend.background = element_rect(fill="NA"),
                    legend.position = "right",
                    axis.text = element_text(size=12 + size_mod),
                    axis.title.x = element_text(size=16 + size_mod, vjust = -1), 
                    axis.title.y = element_text(size=16 + size_mod, vjust = 1),
                    strip.text = element_text(size=12 + size_mod, face = "bold"),
                    strip.background = element_rect(fill="grey", colour = "black"),
                    legend.text = element_text(size=13 + size_mod), 
                    legend.title = element_text(size=17 + size_mod),
                    plot.title = element_text(size=20 + size_mod))

options(DT.options = list(iDisplayLength = 6, searching = FALSE, bLengthChange = FALSE))
```

# Abstract

*Lorem ipsum dolor sit amet, est ad doctus eligendi scriptorem. Mel erat falli ut. Feugiat legendos adipisci vix at, usu at laoreet argumentum suscipiantur. An eos adhuc aliquip scriptorem, te adhuc dolor liberavisse sea. Ponderum vivendum te nec, id agam brute disputando mei.*


# Introduction

N-grams (k-mers) are vectors of $n$ characters derived from input sequences, 
widely used in genomics, transcriptomics and proteomics. Despite the continuous 
interest in the sequence analysis, there are only a few tools tailored for 
comparative n-gram studies. Furthermore, the volume of n-gram data is usually 
very large, making its analysis in \textbf{R} especially challenging. 

The CRAN package \textit{biogram} [@burdukiewicz_biogram:_2015] facilitates 
incorporating n-gram data in the \textbf{R} workflows. Aside from the efficient 
extraction and storage of n-grams, the package offers also a feature selection 
method designed specifically for this type of data. QuiPT (Quick Permutation 
Test) uses several filtering criteria such as information gain (mutual 
information) to choose significant n-grams. To speed up the computation and 
allow precise estimation of small p-values, QuiPT uses analytically derived 
distributions instead of a large number of permutations. In addition to this, 
\textit{biogram} contains tools designed for reducing the dimensionality of the 
amino acid alphabet [@murphy_simplified_2000], further scaling down the 
feature space.

To illustrate the usage of n-gram data in the analysis of biological sequences,  
we present two case studies performed solely in \textbf{R}. The first,   
prediction of amyloids, short proteins associated with the number of clinical   
disorders as Alzheimer's or Creutzfeldt-Jakob’s 
diseases [@fandrich_oligomeric_2012], employs random 
forests [@wright_ranger:_2015] trained on n-grams. The second, detection   
of signal peptides orchestrating an extracellular transport of proteins, 
utilizes more complicated probabilistic framework (Hidden semi-Markov 
model,) but still uses n-gram data for training. 


# Methods

The standard approach for filtering k-mer features is a permutation
test [@fisher1935design] , which verifies if a presence or an absence of a specific k-mer is
related to the label of the sequence. We propose new framework called QuiPT, an exact
version of permutation test, which is more accurate, faster and as customizable,
in terms of measure of dependence we use, as permutation test itself.

## Permutation test

Let us consider target $y=(y_1, ..., y_n)$ 
and matrix of features $X\in M_{n\times p}$. More specifically, as we focus on
a method for filtering k-mer,
we shall focus on one specific feature $x=(x_1, ...., x_n)$. 
When $x$ origins from positioned k-mer analysis 
then it is binary. In this paper we assume that $y$ is also binary.

The standard approach [cite] is to define some measure $C(x,y)$ of dependence 
between $x$ and $y$. Then in permutation test we reshuffle feature and target indpenedently 
large number of times and compute
approximate p-value for how extreme is the dependency between $x$ and $y$ measured with $C(\cdot, \cdot)$.

$$p_{value} \approx \frac{\sum_i^m C(s_i(x), s_i(y))>C(x,y)}{m}$$
where m is number of permutations, and $s_i$ denotes shuffling in i-th
permutation.

## Quick Permutation Test - QuiPT

In this secion we shall outline how one can avoid performing huge number of permutations
and get exact p-values. The main idea of this approach was previously described in 
http://www.cytel.co/hs-fs/hub/1670/file-2600444346-pdf/lib/pubs/exact-inference-for-categorical-data.pdf
and we shall briefly state what is that idea based on, and we show how we managed to speed it up.

Let us consider the contingency table for $y$ and $x$:

| target\\feature | 1 | 0 |
|:---:|:---:|:---:|
| 1 | $n_{1,1}$ | $n_{1,0}$ |
| 0 | $n_{0,1}$ | $n_{0,0}$ |


If probability that target equals 1 is $p$ and probability that feature equals
1 is $q$ and feature and target are independant then each of them has the
following probabilities
$$P((Target, Feature) = (1,1)) = p \cdot q$$
$$P((Target, Feature) = (1,0)) = p \cdot (1-q)$$
$$P((Target, Feature) = (0,1)) = (1-p) \cdot q$$
$$P((Target, Feature) = (0,0)) = (1-p) \cdot (1-q)$$

This means that a target-feature can be described as multinomial distribution.

$$ {n \choose n_{1,1}} (p\cdot q)^{n_{1,1}}
{n - n_{1,1} \choose n_{1,0}} (p\cdot (1-q))^{n_{1,0}}
{n - n_{1,1} - n_{1,0} \choose n_{0,1}} ((1-p)\cdot q)^{n_{0,1}}
{n - n_{1,1} - n_{1,0} -n_{0,1}\choose n_{0,0}} ((1-p)\cdot (1-q))^{n_{0,0}}$$

with additional restriction that $n_{1,\cdot} = n_{1,1} + n_{1,0}$ and
$n_{\cdot, 1} = n_{1,1} + n_{0,1}$ are known and fixed as they describe the number
of ,,ones" for target and feature respectively.

The introduction of these restrictions is not an unnecessary complication, but in
fact simplifies computations, because we can describe whole confusion matrix using
only $n_{1,1}$.

Observe that $n_{1,1}$ is from range $[0,min(n_{\cdot, 1}, n_{1, \cdot})]$.
So we get probability of certain contingency table as conditional distribution,
as impose restrictions on two parameters $n_{\cdot, 1}$ and $n_{1, \cdot}$
We can compute IG for each possible value of $n_{1,1}$ and finally we get
distribution of Information Gain under hypothesis that target and feature
are independant.

Having exact distribution allows us to perform permutation test much quicker as we
no longer need large number of replications. Furthermore, by using
exact test we will get precise values of even in tails of statistic distribution,
which was not guaranteed with random permutations. 
In fact, suppose we want to perform test with $\alpha=10^{-8}$, which is
not uncommon value when we adjust for multiple testing - more on that later.
Even for huge number of permuations $m=10^8$ standard deviation of permutation test estimate
$\frac{p(1-p)}{m}$ is roughly equal true p-value itself.

In the context of k-mer data we can speed up our algorithm even further. Note that
since target $y$ is common for testing all k-mer features, test statistics depends only on
number of positive cases in feature $n_{\cdot, 1}$. 
Though we test millions of features, there are just few distributions
that we need to compute, as usually number of positives in k-mer is small. 
We take advantage of this fact, and therefore complexity of our algorithm
is roughly equal $n*p$.

Furthermore, as we deal with sparse vectors, our implementation in R is optimized by 
using libraries **bit**, which speeds up
computing entropy and information gain, and **slam**, for sparse encoding of data. 
Thanks to those details **biogram** works with even very large data sets, being limited only by
available RAM.

??? GIVE SOME EXAMPLE OF BIG DATA HANDLED ????
 
|  Size|    matrix [bytes] | slam [bytes] |
|-----:|---------:|----:|
| 1e+01|      1000| 1032|
| 1e+04|     80200| 1032|
| 1e+09|   8000200| 1032|
| 1e+16| 800000200| 1032|

Caption: Size of data set (a matrix with the number of records equal to the size) in the memory. matrix is 
a standard R matrix, while slam is the size of the object created using the slam package.
 
### Relationship with Fisher's exact test

Fisher's exact test is a test for independance in contingency table (mostly $2\times 2$).
From derivation provided in [citation Lehmann], it becomes obvious that what we did in previous section is
providing a framework for heuristics in two-tailed Fisher's exact test, for which there is no
right solution.

In fact, QuiPT can give different results than Fisher's exact test as implemented in R.

```{r, fisher_quipt_setup, message=FALSE}
library(biogram)
n11 <- 15
n10 <- 0
n01 <- 30
n00 <- 15

target_feature <- create_feature_target(n11 = n11, n01 = n01, n10 = n10, n00 = n00)
m <- matrix(c(n11,  n10,  n01,  n00), ncol = 2)
fisher.test(x = m, alternative = "t")
test_features(target_feature[, 1], cbind(target_feature[, 2], target_feature[, 2]))
```

Where does the difference come from?

```{r, echo=FALSE}
dist <- distr_crit(target = target_feature[,1], feature = target_feature[,2])
probs <- dhyper(x = 0:(n11+n10), m = n11+n01, n = n10+n00, k = n11+n10)
z.df <- attr(dist, "plot_data")
z.df <- data.frame(cbind(x=0:15,z.df))
```

For $n_{11}=7$ we have higher probability of occurence (`r probs[8]`) smaller than one for $n_{11}=15$ - `r probs[16]`,
which means that Fisher's test is more likely to reject 7 than 15.
However Information Gain for  $n_{11}=15$ equal `r z.df$unsort_criterion[16]` is higher
than Information Gain for  $n_{11}=7$ equal `r z.df$unsort_criterion[8]` which means that QuiPT would be more
likely to reject it.

```{r, rejectionPlot, echo=FALSE, fig.align='center', fig.cap="$n=60, n_{1 \\bullet}=15, n_{\\cdot 1}=45$"}
# order(z.df$unsort_prob)[1:(which.min(cumsum(sort(z.df$unsort_prob))<0.011)-1)]
# order(z.df$unsort_criterion, decreasing = T)[1:(which.min(
#   cumsum(z.df$unsort_prob[order(z.df$unsort_criterion, decreasing = T)])<0.01)-1)]

library(ggplot2)
library(latex2exp)
ggplot(z.df,aes(x=x,y=unsort_prob))+
  geom_density(aes(x=x,y=unsort_prob), color="black", stat = "identity") +
  geom_ribbon(data=z.df[1:7,], aes(ymin=0, ymax=unsort_prob, fill = "Common rejection", alpha=0.2)) +
  geom_ribbon(data=z.df[7:8,], aes(ymin=0, ymax=unsort_prob, fill = "Fisher's exact test", alpha=0.2)) +
  geom_ribbon(data=z.df[15:16,], aes(ymin=0, ymax=unsort_prob, fill = "QuiPT", alpha=0.2)) +
  scale_fill_discrete("Rejection region") + guides(alpha=FALSE) +
  xlab(TeX("n_{11}")) + ylab("Probability") +
  theme(legend.position="bottom") +
  ggtitle("Rejection regions for independence hypothesis at the level 0.01\nfor Fisher's exact test and QuiPT")
```

## Criteria

### Information Gain

##### Definition - entropy
For a discrete random variable we define entropy by:
$$H(X) = - \sum^m p_j log p_j$$
For a Bernoulli r.v. we get a simplified expression
$$H(X) = -p \cdot log(p) - (1-p) \cdot log(1-p)$$

##### Definition -  conditional entropy
For two discrete r.v. X, Y we define average conditional entropy by:
$$H(Y|X) = \sum_j P(X=v_j) H(Y|X=v_j)$$
If X and Y are Bernoulli's then:
$$H(Y|X) = q \cdot H(Y|X=1) + (1-q) \cdot H(Y|X=0)$$

##### Definition - Information gain
$$IG(Y|X) = H(Y) - H(Y|X)$$


### $\chi^2$

### Kullback-Leibler

### Bhattacharyya distance

### Mahalanobis distance

## Encoding distance

To compare the obtained encodings, we introduced the encoding distance, which is a 
measure defining the similarity between two encodings. It assumes the value equal to zero for 
identical encodings and grows with the differences between the encodings. This 
parameter enabled us to select the encodings that were very similar to 
the best-performing one and verify if they also show the good prediction performance.

  We defined the encoding distance as the minimum number of amino acids that needed 
to be moved between subgroups of the encoding \textit{a} to make it identical to 
the encoding \textit{b} (the order of subgroups in encodings and the order of 
amino acids in groups are not important). This measure was further scaled by a 
factor reflecting how much the movement of amino acids between groups altered the mean 
values of physicochemical properties in the groups. 

  To compute the scale factor $s$ for the encoding distance between the encoding 
\textit{a} with $n$ subgroups (enumerated with $i$) and the encoding \textit{b} 
with $m$ subgroups (enumerated with $j$), we first calculated $p_i$ and $q_j$, 
i.e. the mean values of corresponding physicochemical properties of all amino acids for each 
subgroup. 

The factor $s$ between encodings $a$ and $b$ is equal to: 

$$ 
s_{ab} = \sum^n_{i = 1}  \left( \min_{j=1,\dots,m} \; \; \sum^L_{l=1} \sqrt{ 
(p_{i,l} - q_{j,l})^2} \right) 
$$
where $L$ is the number of considered physicochemical properties. Hence, the 
normalizing factor may be interpreted as the minimum change of the mean 
physicochemical properties between two considered encodings.


The encoding distance, as computed per the calc_ed function, between a and b is defined as the minimum number of amino acids that have to be moved between subgroups of encoding to make a identical to b (the order of subgroups in the encoding and amino acids in a group is unimportant).

The encoding distance may be further normalized by a factor reflecting how much moving amino acids between groups altered mean group properties.

```{r, echo = FALSE, message = FALSE, results='asis'}
group2df <- function(group_list, caption = NULL, label = NULL) {
  data.frame(ID = 1L:length(group_list), 
             Groups = sapply(group_list, function(i)
    paste0(toupper(sort(i)), collapse = ", ")))
}

a <- list(`1` = "p", 
          `2` = c("f", "i", "w", "y"), 
          `3` = c("a", "c", "d", "e", "g", "h", "k", "l", "m", "n", "q", "r", "s", "t", "v"))

kable(group2df(a), caption = "Encoding A")
```

```{r, echo = FALSE, message = FALSE, results='asis'}
b <- list(`1` = c("f", "r", "w", "y"), 
          `2` = c("c", "i", "l", "t", "v"), 
          `3` = c("a", "d", "e", "g", "h", "k", "m", "n", "p", "q", "s"))

kable(group2df(b), caption = "Encoding B")
```

The encoding distance between both encodings is `r calc_ed(a, b)`.  

## Normalized encoding distance 

```{r, echo = FALSE, message = FALSE, results='asis'}
data(aaprop)
a_prop <- aaprop[c(22, 211), ]

#b_prop <- aa_nprop[na.omit(traits_table[ao, ]), , drop = FALSE]

# must have unified lists of features

coords_a <- lapply(a, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))
coords_b <- lapply(b, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))

dat_a <- data.frame(enc = "a", do.call(rbind, coords_a), label = paste0("A", 1L:3))
dat_b <- data.frame(enc = "b", do.call(rbind, coords_b), label = paste0("B", 1L:3))

dat <- data.frame(do.call(rbind, lapply(1L:nrow(dat_a), function(id) 
  data.frame(id = id, rbind(do.call(rbind, lapply(1L:3, function(dummy) 
    dat_a[id, , drop = FALSE])),
    dat_b)))), pair = c(paste0("d", 1L:3), paste0("d", 1L:3)))

colnames(dat) <- c("id", "enc", "f1", "f2", "label", "pair")
dat[["id"]] <- paste0("Encoding a\nsubgroup ", dat[["id"]])


ggplot(dat, aes(x = f1, y = f2, colour = pair, label = label)) +
  geom_line() +
  geom_point(aes(x = f1, y = f2, colour = enc), size = 4) + 
  facet_wrap(~ id) + 
  geom_text(aes(x = f1, y = f2, colour = enc, label = label), vjust = 1.5, size = 4) + 
  scale_color_brewer(palette="Dark2", guide = "none") +
  cool_theme
```


The figure above represents the distances between groups of encoding **a** (green dots) and groups of encoding **b** (red dots). The position of the dot defined by mean values of specific properties of all amino acids belonging to the group.


```{r, echo = FALSE, message = FALSE, results='asis'}
data(aaprop)
a_prop <- aaprop[c(22, 211), ]


coords_a <- lapply(a, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))
coords_b <- lapply(b, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))

dat_a <- data.frame(enc = "a", do.call(rbind, coords_a), label = paste0("A", 1L:3))
dat_b <- data.frame(enc = "b", do.call(rbind, coords_b), label = paste0("B", 1L:3))

dat <- data.frame(do.call(rbind, lapply(1L:nrow(dat_a), function(id) 
  data.frame(id = id, rbind(do.call(rbind, lapply(1L:3, function(dummy) 
    dat_a[id, , drop = FALSE])),
    dat_b)))), pair = c(paste0("d", 1L:3), paste0("d", 1L:3)))

colnames(dat) <- c("id", "enc", "f1", "f2", "label", "pair")
dat[["id"]] <- paste0("Encoding a\nsubgroup ", dat[["id"]])


ggplot(dat, aes(x = f1, y = f2, colour = pair, label = label)) +
  geom_line() +
  geom_point(aes(x = f1, y = f2, colour = enc), size = 4) + 
  facet_wrap(~ id) + 
  geom_text(aes(x = f1, y = f2, colour = enc, label = label), vjust = 1.5, size = 4) + 
  scale_color_brewer(palette="Dark2", guide = "none") +
  cool_theme
```


```{r, echo = FALSE, message = FALSE, results='asis'}
tmp <- sapply(coords_a, function(single_coords_a) {
  distances <- sapply(coords_b, function(single_coords_b) 
    #vector of distances between groups
    sqrt(sum((single_coords_a - single_coords_b)^2))
  )
  #c(dist = min(distances), id = unname(which.min(distances)))
  distances
})

colnames(tmp) <- paste0("Enc a, group ", colnames(tmp))
rownames(tmp) <- paste0("Enc b, group ", rownames(tmp))

kable(tmp)
```


For each group in encoding **a**, we choose the minimum distance $d_i$ between the group and any group belonging to the encoding **b**. The normalization factor is equal to the sum of minimum distances for each group in **a**.

$$
n_f = \sum_{i \in 1, 2, 3} \min \left( d_i \right) 
$$

In the case depicted above, $n_f$ is equal to the sum of `r paste0(round(apply(tmp, 2, min), 4), collapse = ", ")`: `r round(sum(apply(tmp, 2, min)), 4)`.

# Simulations

Compute how drastic must be the difference of fractions in both groups.  
Simulation study (motif length, sequence lenth, alphabet length) - sensitivity.

#References

